{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "91580319-8d99-4440-b36e-d8d6ae1a74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.functional import log_softmax\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eaa16ee0-c72a-4afd-a632-a468778047ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_file = \"adult.data\"\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "data = pd.read_csv(data_file, header=None, names=columns, na_values=' ?', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "174088c8-9b21-4dd7-81fb-c7f0d181f38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features batch shape: torch.Size([128, 14])\n",
      "Labels batch shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = [\n",
    "    'workclass', 'education', 'marital-status', 'occupation', \n",
    "    'relationship', 'race', 'sex', 'native-country', 'income'\n",
    "]\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Normalize numerical columns\n",
    "numerical_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = StandardScaler()\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "# Split the data\n",
    "X = data.drop(columns=['income']).values\n",
    "y = data['income'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define PyTorch Dataset\n",
    "class AdultDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = AdultDataset(X_train, y_train)\n",
    "test_dataset = AdultDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,drop_last=True)\n",
    "\n",
    "# Print sample data\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"Features batch shape: {X_batch.shape}\")\n",
    "    print(f\"Labels batch shape: {y_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7df7ff99-ba54-4e75-b468-77b4077c5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)  # Binary classification (2 classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = Classifier(input_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ea4eb6fe-abfc-4eb4-b9e9-f214818df266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4531\n",
      "Epoch 2/20, Loss: 0.3866\n",
      "Epoch 3/20, Loss: 0.3625\n",
      "Epoch 4/20, Loss: 0.3447\n",
      "Epoch 5/20, Loss: 0.3360\n",
      "Epoch 6/20, Loss: 0.3330\n",
      "Epoch 7/20, Loss: 0.3307\n",
      "Epoch 8/20, Loss: 0.3284\n",
      "Epoch 9/20, Loss: 0.3286\n",
      "Epoch 10/20, Loss: 0.3270\n",
      "Epoch 11/20, Loss: 0.3257\n",
      "Epoch 12/20, Loss: 0.3259\n",
      "Epoch 13/20, Loss: 0.3251\n",
      "Epoch 14/20, Loss: 0.3227\n",
      "Epoch 15/20, Loss: 0.3228\n",
      "Epoch 16/20, Loss: 0.3222\n",
      "Epoch 17/20, Loss: 0.3228\n",
      "Epoch 18/20, Loss: 0.3227\n",
      "Epoch 19/20, Loss: 0.3219\n",
      "Epoch 20/20, Loss: 0.3201\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1f8c5209-2e78-436d-af16-992dcfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8536\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "05435ae0-ed5a-49eb-81ee-bc5961c8293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "#create a batch for code test\n",
    "first_batch=next(iter(test_loader))\n",
    "inputs,labels=next(iter(test_loader))\n",
    "print(inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7996c681-0f96-496f-92d6-beea4c244c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM: Function to Generate Adversarial Examples\n",
    "\n",
    "def generate_adversarial_example(X, y, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Generates an adversarial example using the Fast Gradient Sign Method (FGSM).\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        criterion: Loss function.\n",
    "        X: Input features (tensor).\n",
    "        y: True label (tensor).\n",
    "        epsilon: Perturbation factor.\n",
    "\n",
    "    Returns:\n",
    "        Adversarial example.\n",
    "    \"\"\"\n",
    "    # Ensure the input tensor requires gradient\n",
    "    X.requires_grad = True\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    \n",
    "    # Backward pass to compute gradients\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Get the gradient sign\n",
    "    gradient = X.grad.data\n",
    "    perturbation = epsilon * gradient.sign()\n",
    "\n",
    "    # Add perturbation to input\n",
    "    adversarial_X = X + perturbation\n",
    "\n",
    "    # Clip the adversarial example to ensure it's within valid input bounds\n",
    "    adversarial_X = torch.clamp(adversarial_X, min=0, max=1)  # Adjust bounds if necessary\n",
    "    return adversarial_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a936d26e-a079-475f-988f-b194b7e8e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0742, -0.3212], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(inputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5f587aaa-0c32-4b59-9b02-cd5f1bf8bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 14])\n"
     ]
    }
   ],
   "source": [
    "adversarial_X=generate_adversarial_example(inputs,labels,0.1)\n",
    "print(adversarial_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "69336f1d-115b-47a7-b49e-ae6621eb2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(h, r, adversarial_X,theta_2, theta_1, y_batch, X_batch):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the target value based on the given formula and iterations.\n",
    "\n",
    "    Args:\n",
    "        h (float): Scalar multiplier.\n",
    "        r (float): Scalar parameter.\n",
    "        theta_2 (float): Coefficient for the logarithmic term.\n",
    "        theta_1 (float): Coefficient for the L2 distance term.\n",
    "        criterion (function): Loss function to compute loss between predictions and true labels.\n",
    "        x_ (torch.Tensor): Input tensor.\n",
    "        y_batch (torch.Tensor): Batch of true labels.\n",
    "        adversarial_X (torch.Tensor): Adversarial input tensor.\n",
    "        x_batch (torch.Tensor): Batch of original input tensors.\n",
    "\n",
    "    Returns:\n",
    "        float: Computed target value.\n",
    "    \"\"\"\n",
    "    inner = 0.0\n",
    "    for i in range(128):  # Assuming 128 batch size\n",
    "        #print(X_batch[i].shape,y_batch[i].shape)\n",
    "        loss = criterion(model(X_batch[i]), y_batch[i])\n",
    "        #print(X_batch[i].shape,y_batch[i].shape)\n",
    "        #print(loss)# Compute the loss\n",
    "        l2_dist = l2_distance(adversarial_X[i], X_batch[i])  # Compute the L2 distance\n",
    "        inner += torch.exp(h * loss + theta_1 * l2_dist)  # Update the inner value\n",
    "    \n",
    "    target = h * r - theta_2 * torch.log(inner)\n",
    "\n",
    "\n",
    "    \n",
    "    return target\n",
    "\n",
    "def l2_distance(x, z, dim=None, keepdim=False):\n",
    "    \"\"\"\n",
    "    Computes the L2 distance (Euclidean distance) between two vectors or tensors.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): First vector or tensor.\n",
    "        z (torch.Tensor): Second vector or tensor.\n",
    "        dim (int, optional): Dimension along which to compute the distance for batched data. Default is None.\n",
    "        keepdim (bool, optional): Whether to retain reduced dimensions. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: L2 distance between x and z.\n",
    "    \"\"\"\n",
    "    return torch.norm(x - z, p=2, dim=dim, keepdim=keepdim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ddab5e02-cb2b-42d6-8647-8290225a3e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss (f(h)): 21.1794, h: 0.8588\n",
      "Epoch 11/20, Loss (f(h)): 20.9036, h: -0.0871\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 1: Define the parameter 'h' that we want to optimize\n",
    "h = torch.tensor([1.0], requires_grad=True)  # Initial guess for h, requires gradients\n",
    "\n",
    "# Step 2: Set up the Adam optimizer to optimize 'h'\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "optimizer = optim.Adam([h], lr=learning_rate)  # Adam optimizer for parameter 'h'\n",
    "\n",
    "# Step 3: Define the function we want to minimize (f(h))\n",
    "def f(h):\n",
    "    return -compute_h(h,0.2,adversarial_X,1,0.4,labels,inputs)  # Quadratic function\n",
    "\n",
    "# Step 4: Training loop to minimize 'f(h)' using Adam\n",
    "epochs = 20  # Number of iterations\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "       \n",
    "    \n",
    "       optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "       for k in range(3):\n",
    "           adversarial_X=generate_adversarial_example(X_batch,y_batch,0.1)\n",
    "        \n",
    "\n",
    "    # Compute the output of the function (this is f(h))\n",
    "       f=-compute_h(h,0.2,adversarial_X,1,0.4,y_batch,X_batch)\n",
    "       \n",
    "       output = f\n",
    "\n",
    "    # Compute the loss (the function value itself in this case)\n",
    "       loss = output\n",
    "\n",
    "    # Backpropagation to compute gradients\n",
    "       loss.backward(retain_graph=True)\n",
    "\n",
    "    # Update 'h' using the Adam optimizer\n",
    "       optimizer.step()\n",
    "\n",
    "    # Print progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss (f(h)): {loss.item():.4f}, h: {h.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "55f784d5-421c-4132-a16c-ba69f59f5a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6414e-22, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        3.6974e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6060e-01,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7391e-08, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 2.0332e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5705e-02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(get_weights(-f,adversarial_X,0.4,1,X_batch,y_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a09b39dd-d676-45bc-92d9-015090fbc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(h,adversarial_X,theta_1,theta_2,X_batch,y_batch) :\n",
    "#try to calculate the weights\n",
    "    weights=torch.zeros(128)\n",
    "    for i in range(128):\n",
    "        loss = criterion(model(X_batch[i]), y_batch[i])\n",
    "        l2_dist = l2_distance(adversarial_X, X_batch[i])\n",
    "        l=h*loss-theta_1*l2_dist\n",
    "        \n",
    "        weights[i]=torch.exp(l/theta_2)\n",
    "\n",
    "    total_weights=weights.sum()\n",
    "    weights=weights/total_weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602115ca-f8b3-4cd8-a9fe-6583c490cf70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
